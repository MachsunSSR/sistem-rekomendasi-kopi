{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preferensi</th>\n",
       "      <th>Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Kopisang (es kopi susu dengan campuran peris...</td>\n",
       "      <td>minum bahan dasar kopi inginya manis milik dingin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['MangoBoost (campuran sirup mangga dengan sus...</td>\n",
       "      <td>minum dingin bahan dasar susu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Chocobanana (es coklat dengan campuran peris...</td>\n",
       "      <td>minum bahan dasar coklat milik perisa dingin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Tubruk (seduhan dari gilingan kasar dari kop...</td>\n",
       "      <td>minum bahan dasar kopi panas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Ice Lychee Tea (teh leci dengan buah leci)']</td>\n",
       "      <td>menginkan minum bahan dasar teh dingin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Preferensi  \\\n",
       "0  ['Kopisang (es kopi susu dengan campuran peris...   \n",
       "1  ['MangoBoost (campuran sirup mangga dengan sus...   \n",
       "2  ['Chocobanana (es coklat dengan campuran peris...   \n",
       "3  ['Tubruk (seduhan dari gilingan kasar dari kop...   \n",
       "4     ['Ice Lychee Tea (teh leci dengan buah leci)']   \n",
       "\n",
       "                                             Cleaned  \n",
       "0  minum bahan dasar kopi inginya manis milik dingin  \n",
       "1                      minum dingin bahan dasar susu  \n",
       "2       minum bahan dasar coklat milik perisa dingin  \n",
       "3                       minum bahan dasar kopi panas  \n",
       "4             menginkan minum bahan dasar teh dingin  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"cleaned.csv\")\n",
    "df = df.drop(['Deskripsi'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF IDF From Scratchl\n",
    "import math\n",
    "\n",
    "class CustomTFIDFVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.idf = {}\n",
    "        self.doc_count = 0\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        self.doc_count = len(documents)\n",
    "\n",
    "        # Step 1: Calculate Term Frequencies (TF)\n",
    "        tf_matrix = []\n",
    "        for doc in documents:\n",
    "            tf_doc = {}\n",
    "            for word in doc.split():\n",
    "                self.vocab.add(word)\n",
    "                tf_doc[word] = tf_doc.get(word, 0) + 1\n",
    "            tf_matrix.append(tf_doc)\n",
    "\n",
    "        # Step 2: Calculate Inverse Document Frequencies (IDF)\n",
    "        for word in self.vocab:\n",
    "            doc_count_with_word = sum(1 for doc in documents if word in doc)\n",
    "            self.idf[word] = math.log(self.doc_count / (doc_count_with_word + 1))\n",
    "\n",
    "        # Step 3: Compute TF-IDF Scores\n",
    "        tfidf_matrix = []\n",
    "        for tf_doc in tf_matrix:\n",
    "            tfidf_doc = {}\n",
    "            for word, tf in tf_doc.items():\n",
    "                tfidf_doc[word] = tf * self.idf[word]\n",
    "            tfidf_matrix.append(tfidf_doc)\n",
    "\n",
    "        return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# documents = [\n",
    "#     \"this is the first document\",\n",
    "#     \"this document is the second document\",\n",
    "#     \"and this is the third one\",\n",
    "#     \"is this the first document\",\n",
    "# ]\n",
    "\n",
    "# custom_tfidf = CustomTFIDFVectorizer()\n",
    "# tfidf_matrix = custom_tfidf.fit_transform(documents)\n",
    "\n",
    "# # Print TF-IDF scores for the first document\n",
    "# for word, tfidf_score in tfidf_matrix[0].items():\n",
    "#     print(f\"{word}: {tfidf_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF to vectorize the words in Cleaned columns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = CustomTFIDFVectorizer()\n",
    "X = tfidf.fit_transform(df[\"Cleaned\"])\n",
    "y = df[\"Preferensi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import decision_tree scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Define a stopping criterion\n",
    "        def should_stop(y):\n",
    "            return len(set(y)) == 1 or (self.max_depth is not None and len(y) <= self.max_depth)\n",
    "\n",
    "        # Define a function to find the best split\n",
    "        def find_best_split(X, y):\n",
    "            best_gini = float('inf')\n",
    "            best_split = None\n",
    "            for feature in range(X.shape[1]):\n",
    "                unique_values = np.unique(X[:, feature])\n",
    "                for value in unique_values:\n",
    "                    left_mask = X[:, feature] <= value\n",
    "                    right_mask = ~left_mask\n",
    "                    gini = (np.sum(left_mask) * gini_impurity(y[left_mask]) +\n",
    "                            np.sum(right_mask) * gini_impurity(y[right_mask])) / len(y)\n",
    "                    if gini < best_gini:\n",
    "                        best_gini = gini\n",
    "                        best_split = (feature, value)\n",
    "            return best_split\n",
    "        \n",
    "        def gini_impurity(y):\n",
    "            if len(y) == 0:\n",
    "                return 0\n",
    "            p = np.bincount(y) / len(y)\n",
    "            return 1 - np.sum(p ** 2)\n",
    "\n",
    "        # Define a recursive function to build the tree\n",
    "        def build_tree(X, y, depth):\n",
    "            if should_stop(y) or (self.max_depth is not None and depth == self.max_depth):\n",
    "                return {'class': np.bincount(y).argmax()}\n",
    "            feature, value = find_best_split(X, y)\n",
    "            if feature is None:\n",
    "                return {'class': np.bincount(y).argmax()}\n",
    "            left_mask = X[:, feature] <= value\n",
    "            right_mask = ~left_mask\n",
    "            left_tree = build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "            right_tree = build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "            return {\n",
    "                'feature': feature,\n",
    "                'value': value,\n",
    "                'left': left_tree,\n",
    "                'right': right_tree\n",
    "            }\n",
    "\n",
    "        self.tree = build_tree(X, y, 0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        def predict_single(x, node):\n",
    "            if 'class' in node:\n",
    "                return node['class']\n",
    "            if x[node['feature']] <= node['value']:\n",
    "                return predict_single(x, node['left'])\n",
    "            else:\n",
    "                return predict_single(x, node['right'])\n",
    "\n",
    "        return np.array([predict_single(x, self.tree) for x in X])\n",
    "\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, max_features=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Step 1: Bootstrap Sampling\n",
    "            sample_indices = np.random.choice(range(len(X)), size=len(X), replace=True)\n",
    "            X_sampled = X[sample_indices]\n",
    "            y_sampled = y[sample_indices]\n",
    "\n",
    "            # Step 2: Feature Selection\n",
    "            if self.max_features is not None:\n",
    "                feature_indices = np.random.choice(range(X.shape[1]), size=self.max_features, replace=False)\n",
    "                X_sampled = X_sampled[:, feature_indices]\n",
    "\n",
    "            # Step 3: Build Decision Trees\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            tree.fit(X_sampled, y_sampled)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Step 4: Aggregate Predictions\n",
    "        predictions = np.zeros((len(X), len(self.trees)))\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            predictions[:, i] = tree.predict(X)\n",
    "\n",
    "        # Aggregate predictions from all trees\n",
    "        final_predictions = np.mean(predictions, axis=1)\n",
    "        return final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming tfidf_matrix is in the format you provided\n",
    "# Convert it to a 2D numpy array\n",
    "X_train = np.array([[sample.get(word, 0) for word in X[0]] for sample in X])\n",
    "\n",
    "# Assuming y_train is a list of labels\n",
    "# Convert it to a numpy array if it's not already\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y)\n",
    "\n",
    "# Create a RandomForestClassifier instance\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "\n",
    "# Train the random forest\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf.predict(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
